{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models.py\n",
    "## What It Contains:\n",
    "\n",
    "Neural Network Architectures: Defines the core RL agent architecture (e.g., BestPokerModel), which uses a dueling DQN structure with noisy linear layers and residual blocks.\n",
    "\n",
    "Custom Layers & Blocks: Implements helper components like NoisyLinear (for exploration) and ResidualBlock (for improved gradient flow).\n",
    "\n",
    "Checkpoint Conversion: Provides a utility function (convert_half_to_full_state_dict) to convert model checkpoints from a reduced (half-poker) encoding to the full input dimensions used by the agent.\n",
    "\n",
    "\n",
    "# envs.py\n",
    "## What It Contains:\n",
    "\n",
    "BaseFullPokerEnv: A base class implementing the complete poker game logic including dealing cards, betting rounds, stage progression, hand evaluation, and opponent actions.\n",
    "\n",
    "TrainFullPokerEnv: An extension of the base environment tailored for training. It adds features like tracking all-in actions and modified reward computation.\n",
    "\n",
    "Helper Functions: Contains a simple poker hand evaluator and functions to compute the belief state for each player.\n",
    "\n",
    "# utils.py\n",
    "## What It Contains:\n",
    "\n",
    "Observation Encoders: Functions (encode_obs and encode_obs_eval) that transform the raw game observations into numerical state vectors suitable for the RL agent.\n",
    "\n",
    "Epsilon Scheduling: Implements an epsilon_by_frame function that calculates the current epsilon value for epsilon-greedy exploration.\n",
    "\n",
    "Replay Buffer: A class to store and sample experience tuples (state, action, reward, next state, done) during training.\n",
    "\n",
    "Logging Helper: A simple logging function (log_decision) to print debug or decision-related messages when enabled.\n",
    "\n",
    "# train_rl.py\n",
    "## What It Contains:\n",
    "\n",
    "Training Loop: The main training script for the RL agent.\n",
    "\n",
    "RL Agent Setup: Instantiates the BestPokerModel (from models.py) and uses the training environment (TrainFullPokerEnv from envs.py).\n",
    "\n",
    "Experience Replay and Target Network: Uses a replay buffer and periodically updates a target network to stabilize training.\n",
    "\n",
    "Checkpointing: Saves model checkpoints and training metrics periodically to allow resuming training later.\n",
    "\n",
    "## How to Run:\n",
    "You can start training by running the training command (either directly or via the main entry point described below). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py train --checkpoint checkpoints/optional_checkpoint.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Omit --checkpoint if you want to start from scratch.)_\n",
    "\n",
    "\n",
    "# simulate.py (or evaluate.py)\n",
    "## What It Contains:\n",
    "\n",
    "Simulation/Evaluation Loop: This script sets up a poker environment (BaseFullPokerEnv) for running simulation episodes.\n",
    "\n",
    "Greedy Policy: Uses the trained agent (loaded from a checkpoint) and runs episodes using a deterministic, greedy policy (i.e., no exploration).\n",
    "\n",
    "Output: Prints episode rewards and additional game outcome details (like winners, scores, etc.) for analysis.\n",
    "\n",
    "## How to Run:\n",
    "You can evaluate a trained model by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py simulate --checkpoint checkpoints/optional_checkpoint.pt --episodes 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Omit --checkpoint if you want to start from scratch.)_\n",
    "\n",
    "\n",
    "# main.py\n",
    "## What It Contains:\n",
    "\n",
    "Unified Entry Point: Acts as the central script that ties the project together. It uses subcommands to either launch the training loop or the simulation/evaluation loop.\n",
    "\n",
    "Subcommand Handling:\n",
    "\n",
    "The train subcommand calls the training routine from train_rl.py.\n",
    "\n",
    "The simulate subcommand forwards arguments to the simulation script (simulate.py).\n",
    "\n",
    "## How to Run:\n",
    "\n",
    "### For Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py train --checkpoint checkpoints/optional_checkpoint.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Omit --checkpoint if you want to start from scratch.)_\n",
    "\n",
    "### For Simulation/Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py simulate --checkpoint checkpoints/optional_checkpoint.pt --episodes 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_random.py\n",
    "\n",
    "## What It Contains:\n",
    "\n",
    "Training Loop:\n",
    "This script sets up a full training loop for the RL agent using the BestPokerModel architecture and the TrainFullPokerEnv environment. It is designed to train the agent via an epsilon-greedy strategy, experience replay, and target network updates.\n",
    "\n",
    "Random Opponent Policy:\n",
    "A random action policy is explicitly assigned to one of the opponent agents (e.g., opponent with ID 1). This introduces stochastic behavior into the training environment, helping to simulate more varied and unpredictable opponent actions.\n",
    "\n",
    "Experience Replay and Checkpointing:\n",
    "The script uses a replay buffer to store experiences and periodically updates a target network to stabilize training. It also saves model checkpoints and training metrics to disk for future resumption or analysis.\n",
    "\n",
    "## How to Run:\n",
    "\n",
    "You can start training the agent with a random opponent by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python main.py train_random --checkpoint checkpoints/optional_checkpoint.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Omit --checkpoint if you want to start from scratch.)_\n",
    "\n",
    "# Performance Metrics\n",
    "\n",
    "**Episode Reward:** The cumulative reward obtained in each episode.\n",
    "\n",
    "**Average Reward:** Typically computed over the last 100 episodes.\n",
    "\n",
    "**Epsilon Value:** The current exploration rate used in the epsilon-greedy policy.\n",
    "\n",
    "These metrics are printed to the console at regular intervals (e.g., every 100 episodes) and periodically saved to a CSV file (e.g., every 10,000 episodes) in the checkpoint directory. This logging helps monitor training performance over time and facilitates analysis or debugging.\n",
    "\n",
    "\n",
    "# Stop or Kill Script\n",
    "\n",
    "Run this line of code to quit the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!taskkill /F /IM python.exe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
