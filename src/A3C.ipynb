{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C (Asynchronous Advantage Actor-Critic algorithm)\n",
    "This is basically Advantage Actor-Critic but with multiple parallel agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from Environment import TexasHoldem6PlayerEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Actor-Critic Network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker function for parallel training\n",
    "def worker(global_model, optimizer, rank, env_name, gamma, update_steps):\n",
    "    env = gym.make(env_name)\n",
    "    local_model = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
    "    local_model.load_state_dict(global_model.state_dict())\n",
    "    \n",
    "    while True:\n",
    "        state, _ = env.reset()  # state is initialized\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        log_probs, values, rewards = [], [], []\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for _ in range(update_steps):\n",
    "            probs, value = local_model(state)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            next_state, reward, done, _, _ = env.step(action.item())  #done\n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = torch.tensor(next_state, dtype=torch.float32)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        print(f\"Worker {rank}: Episode Reward = {episode_reward}\") \n",
    "        \n",
    "       #Compute advantages and returns after done is defined \n",
    "        R = 0 if done else local_model(state)[1].item()\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        values = torch.cat(values)\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        \n",
    "        advantage = returns - values\n",
    "        \n",
    "        # Compute loss\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "        loss = actor_loss + 0.5 * critic_loss\n",
    "        \n",
    "        # Update global model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for global_param, local_param in zip(global_model.parameters(), local_model.parameters()):\n",
    "            global_param._grad = local_param.grad\n",
    "        optimizer.step()\n",
    "        \n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "        print(f\"Worker {rank}: Actor Loss = {actor_loss.item():.4f}, Critic Loss = {critic_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment (Only Needs to be Done Once)\n",
    "try:\n",
    "    gym.envs.registration.register(\n",
    "        id=\"TexasHoldem6PlayerEnv-v0\",  \n",
    "        entry_point=\"Environment:TexasHoldem6PlayerEnv\",\n",
    "    )\n",
    "except gym.error.Error:\n",
    "    pass  # Ignore if it's already registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Main function to launch parallel training\n",
    "def train_a3c(env_name=\"TexasHoldem6PlayerEnv-v0\", num_workers=4, gamma=0.99, update_steps=50):\n",
    "    env = gym.make(env_name)\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    global_model = ActorCritic(input_dim, action_dim)\n",
    "    global_model.share_memory()\n",
    "    optimizer = optim.Adam(global_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    processes = []\n",
    "    for rank in range(num_workers):\n",
    "        p = mp.Process(target=worker, args=(global_model, optimizer, rank, env_name, gamma, update_steps))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        mp.set_start_method(\"spawn\")\n",
    "    except RuntimeError:\n",
    "        pass  \n",
    "    train_a3c()\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env_name=\"TexasHoldem6PlayerEnv-v0\", model_path=\"trained_a3c_model.pth\"):\n",
    "    env = gym.make(env_name)\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    model = ActorCritic(input_dim, action_dim)\n",
    "    model.load_state_dict(torch.load(model_path))  # Load trained weights\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    print(\"\\nStarting Test Episode...\\n\")\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            probs, _ = model(state)  # Get action probabilities\n",
    "            action = torch.argmax(probs).item()  # Choose best action\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)  # Take action\n",
    "        total_reward += reward  # Accumulate reward\n",
    "        state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "    print(f\"\\nTest Completed: Total Reward = {total_reward}\")\n",
    "\n",
    "    test_agent() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
